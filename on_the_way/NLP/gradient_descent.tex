%Gradient descent--梯度下降算法
\section{梯度下降}
\subsection{实际问题}
假定你是一个拥有餐厅专营权的CEO，现在打算到其它城市开发市场。问题的关键是不知道选择哪个城市。不过好在你有城市的人口和收益的数据，以此为根据来选择在哪个城市发展。

\subsection{问题分析}
问题的关键在于通过已知的城市人口和收益的数据来分析二者的关系。换言之，就是通过一组数据来分析（一个或多个）自变量和因变量的关系，也就是我们通常所说的\textbf{数据拟合}。解题的思路如下：
\begin{enumerate}
\item 作出数据的散点图，根据散点图猜测自变量和因变量的函数关系(线性回归/多项式回归等)\textcolor{red}{[Model Selection]}
\item 求解模型的参数(梯度下降GD/正规方程NE)\textcolor{red}{[Train]}
\item 检验猜测的函数关系的准确度(代价函数$J(\theta_0,\theta_1)$)\textcolor{red}{[Test]}
\end{enumerate}

\subsection{数学模型}
数据的散点图如下\ref{gd:figure:scatter},结合实际的经验可以大胆猜测城市人口和收益之间存在线性关系,即$h_{\theta}(x)=\theta_0+\theta_1*x$.
\begin{figure}[!htbp]
	\centering
	\includegraphics[scale=0.7]{figs/gd_scatter.eps} 
	\caption{城市人口和收益的关系散点图}    	\label{gd:figure:scatter}
\end{figure}
为确定$\Theta=[\theta_0,\theta_1]$,我们利用梯度下降算法使得代价函数最小.此时,该实际问题变成如下数学问题:

\begin{eqnarray}
&Hypothesis: & h_{\theta}(x)=X^T*\Theta=
\theta_0+\theta_1*x_1+\theta_2*x_2+\cdots+\theta_n*x_n \nonumber \\ 
&Parameters: & \Theta = [\theta_0,\theta_1,\cdots,\theta_n] \nonumber \\
&Cost Function:& J(\Theta)=
\frac{1}{2*m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2 \nonumber \\
&Goal:& \min_{\Theta} J(\Theta) \nonumber \\
&Method:&  Gradient \quad Descent  \nonumber 
\end{eqnarray}
其中,$X$是$m*(n+1)$维的矩阵,$m$代表训练样本的个数,$n$代表样本特征的个数,这里$+1$是给每个样本添加1这个特征,对应$\theta_0$这个偏差权重,$\Theta$是一个$n+1$向量,代表每个特征的权重.


\subsection{梯度下降算法}
梯度下降算法是最小化风险函数/损失函数的一种常用方法.对于只有一个峰值的代价函数,梯度下降总是能找到全局最优解,不过对于多峰模型,很有可能梯度下降最总的结果是局部最优.梯度下降算法一般有两个,分别是批量梯度下降算法(Bath GD)和随机梯度下降算法(Stochastic GD).本例中,由于上面的数据集并不是很大,所以对于上面的代价函数,采用批量梯度下降和随机梯度下降算法都可以.
\subsubsection{批量梯度下降算法}
批量梯度下降算法的解题思路如下:
\begin{enumerate}
\item 将$J(\Theta)$对$\theta$求偏导,得到每个$\theta$对应的梯度
\begin{equation}
\frac{\partial J(\Theta)}{\partial \theta_j} = 
\frac{1}{m} \sum_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})*x_j^{(i)}
\end{equation}
\item 由于要最小化代价函数,所以按每个参数$\theta$的梯度负方向来更新每个$\theta$
\begin{equation}
\theta_j = \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})*x_j^{(i)}
\end{equation}
\end{enumerate}
由上面的公式可知,它得到的是一个全局最优解,但是每迭代一步,都要用到训练集所有的数据,其算法的伪代码如下\ref{code:bathgd}.
\begin{algorithm}[h]
\caption{Bath Gradient Descent}
\label{code:bathgd}
\begin{algorithmic}[1]
\Repeat
\State	$\theta_j \gets \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})*x_j^{(i)}$ 
\Until{$\Delta J(\Theta) < C$}
\end{algorithmic}
\end{algorithm}

\subsubsection{随机梯度下降算法}
如果$m$很大,那么这种迭代的速度可想而知,在此基础上就有随机梯度下降.而随机梯度下降算法和批量梯度下降算法用的代价函数一样,不过对$\Theta$的更新有区别,随机梯度下降算法并没有取所有的训练样本,而是取其中的某个样本.其更新表达式如下
\begin{equation}
\theta_j = \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})*x_j^{(i)}
\end{equation}
由上面的公式可知,随机梯度下降是通过每个样本来迭代更新一次,如果样本量很大的情况(几十万),那么可能只用其中几万条或者几千条的样本就会获得最优解,对比上面的批量梯度下降,迭代一次需要用到几十万训练样本,一次迭代不可能能获得最优,那么就得迭代多次,非常耗时间.但是随机梯度下降有个问题就是每次并不是向着整体最优化方向的,不过最终的结果往往是在全局最优解附近.其算法的伪代码如下\ref{code:stocgd}.
\begin{algorithm}[h]
\caption{Stochastic Gradient Descent}
\label{code:stocgd}
\begin{algorithmic}[1]
\State $i \gets 1$
\While{$i < m | \Delta J(\Theta) < C$}
\State	$\theta_j \gets \theta_j - \frac{\alpha}{m} (h_{\theta}(x^{(i)})-y^{(i)})*x_j^{(i)}$ 
\State $i \gets i+1$
\EndWhile
\end{algorithmic}
\end{algorithm}


\section{实验和结果}
\subsection{学习率的选择}
在梯度下降算法中,学习率$\alpha$是个很关键的量,它决定了下降算法每次下降的步长.当$\alpha$很小时,权重参数$\Theta$每次下降的步长就很小,那么它收敛到全局最小值所要走的步数要增多;当$\alpha$很大时,权重参数$\Theta$走的步长很大,很容易错过全局最小值那个点,然后在其周围震荡,更有甚者代价会越来越大.在不同的学习率$\alpha$下,代价$J$随循环次数的变化如下图所示\ref{gd:figure:learingRate}.
\begin{figure}[!htbp]
	\centering
	\includegraphics[scale=0.405]{figs/gd_learningRate.eps} 
	\caption{学习率对下降算法的影响}    
	\label{gd:figure:learingRate}
\end{figure}


从前两幅图中,可以看出随着$\alpha$的增大,代价$J$的下降也更加迅速(三幅图中y轴的分度值和范围都不一样).而当$\alpha=0.03$时,代价却越来越大.一般学习率的选取从0,01开始,*3或/3进行寻找合适的$\alpha$,这里就可以选$\alpha=0.01$作为合适的学习率.


\subsection{特征标准化}
对于不同的特征,他们的取值范围是不一样的.当不同的特征的取值范围相差很大时,那么对应的权值参数也会相差很大,这就导致了在画代价和权值参数的等值线图表时,那个图表是椭圆形,而且非常扁.这样用梯度下降时迭代的步数就会很多,那么算法耗时将更长.所以提出特征标准化.它是将所有的特征的取值范围都映射到$[-1,1]$区间,其均值为0.基本思想就是对每个特征的数据进行如下变换
\begin{equation}
X_{norm}=\frac{(X-\mu)}{\sigma}
\end{equation}
其中X是$m*1$的向量,$m$代表样本数目,$\mu$代表该特征的均值,$\sigma$代表该特征的标准差.特征的标准化可以加快梯度下降算法的收敛速度,具体表现如下图\ref{gd:figure:normalization}.
\begin{figure}[!htbp]
	\centering
	\includegraphics[scale=0.5]{figs/gd_normalization.eps} 
	\caption{标准化对下降算法的影响}    
	\label{gd:figure:normalization}
\end{figure}


从图中只能看出,标准化后的等值线图是一个个同心圆构成的,但是由附件中的$normalization.m$代码可以的到,原始数据在迭代了3000次后代价还没有收敛到全局最小值,但是标准化后的数据在迭代了1387次后代价就已经收敛到全局最小值了.由此可见特征的标准化是能够加速梯度下降算法的收敛的.











